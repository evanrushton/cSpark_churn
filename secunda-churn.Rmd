---
title: "Secunda: Churn"
author: "Evan Rushton"
date: "10/25/2018"
output: html_document
editor_options: 
  chunk_output_type: console
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(BTYD)
library(lubridate)
library(scales)
theme_set(theme_light())
options(scipen = 5)
```

Within a ~4 month time window, (2/01/18 to 06/01/18), user AppOpen behavior decays at varying rates.

Can a model predict how likely someone will churn based on AppOpen data from a subscription service?

### First we load and clean the data
```{r}
app_opens <- read.csv('./Data/NBDparetoFebMay.csv')

# Convert time in ms to dates by day
names(app_opens)[2] <- "date"
app_opens$date <- format(as.POSIXct(app_opens$date / 1000, origin = "1970-01-01", tz = "America/Los_Angeles"), "%Y%m%d")
app_opens$date <- as.Date(app_opens$date, "%Y%m%d")
```

# Exploration

### Check NA values
```{r}
sapply(app_opens, function(y) sum(length(which(is.na(y))))) # None - nice
```

### How many users were active?

```{r}
length(unique(app_opens$uid))
max(app_opens$date) - min(app_opens$date)
```

31,230 unique users across 121 days (4 months).

### Outlier Processing

The frequency of AppOpen events for each user can be made by table() or count().

```{r}
table <- table(app_opens$uid) 
sort(table, decreasing = TRUE)
```

Two users have over 1000 AppOpen events and are either power users or internal QA subs. Our analysis is better off without them.

```{r}
# Processing Step
counts <- app_opens %>% 
  count(uid, sort=TRUE) %>% 
  filter(n < 1000) 
```

### How often are users opening the app?

```{r}
# Histogram
counts %>%  
  filter(n < 100) %>% 
  ggplot(aes(n)) +
  geom_histogram()

# Boxplot
counts %>% 
  filter(n < 30) # still 26,000 users with less than 30 AppOpen events
  ggplot(aes(y=n)) +
  geom_boxplot()
  
# Descriptives
library(psych)
describe(counts$n)

d <- density(counts$n)
plot(d, main="Kernel Density of AppOpen events per User")
```

The distribution is skewed right with a long tail and the median is 8 AppOpen events. The mean of 18 is an artifact of the skewed distribution and 8 is a better approximation of the center.

Some additional processing of the data allows for us to dig into user behavior a bit.

```{r}
# Create a users table
users <- unique(app_opens$uid) %>% 
  as.data.frame() %>% 
  mutate(id = seq.int(length(unique(app_opens$uid)))) # Give each an integer id
names(users)[1] <- "uid"

app_opens <- app_opens %>% 
  group_by(uid) %>% 
  mutate(First = min(date),
            Last = max(date), 
            duration = difftime(max(date), min(date), unit='day') + 1,
            active = max(date) == as.Date('2018-06-01'))

users <- merge(users, app_opens, by = "uid", all.y = TRUE) %>% 
  arrange(uid, date)
```

What duration do users open the app over?

```{r}
app_opens %>% 
  mutate(duration = )
  geom_histogram(binwidth=1)
```


Slight downward trend in appOpen events by date in Spring

```{r}
count_date %>% 
  ggplot(aes(x=x, y=freq)) +
  geom_point() +
  geom_smooth(method = 'lm' )
```

### NBD/Pareto

We can assign integer values to unique users with a join. This integer key is used by the NBT/Pareto model.

```{r}
app_opens <- merge(app_opens, users, by = "uid") %>% 
  arrange(uid, date)
```

```{r}
elogSpring <- app_opens[, c("id", "date")]
names(elogSpring)[1] <- "cust"
elogSpringM <- dc.MergeTransactionsOnSameDate(elogSpring)
```

Check the shape of time between opens to ensure NBD/Pareto is a good fit

```{r}
openFreq <- ddply(elogSpringM, .(cust), summarize, 
                      daysBetween = as.numeric(diff(date)))

ggplot(openFreq,aes(x=daysBetween))+
  geom_histogram(fill="orange")+
  xlab("Time between opens (days)")
```

Merging the counts on a single day happens

```{r}
elogSpring$date<-as.POSIXct(elogSpring$date)
# splitting data
(end.of.cal.period <- min(elogSpring$date)+(max(elogSpring$date)-min(elogSpring$date))/2)
dataSpring <- dc.ElogToCbsCbt(elogSpring, per="day", # merged or not give same result
                        T.cal=end.of.cal.period,
                        statistic = "freq") 
cal2.cbs <- as.matrix(dataSpring[[1]][[1]])
str(cal2.cbs)
```

```{r}
#Parameter estimation
(params2 <- pnbd.EstimateParameters(cal2.cbs))
(LL <- pnbd.cbs.LL(params2, cal2.cbs))
# it is a good idea to make a few more estimates to see if they converge
p.matrix <- c(params2, LL)
for (i in 1:5) {
  params2 <- pnbd.EstimateParameters(cal2.cbs, params2)
  LL <- pnbd.cbs.LL(params2, cal2.cbs)
  p.matrix.row <- c(params2, LL)
  p.matrix <- rbind(p.matrix, p.matrix.row)
}

p.matrix
(params2 <- p.matrix[dim(p.matrix)[1],1:4])

param.names <- c("r", "alpha", "s", "beta")

LL <- pnbd.cbs.LL(params2, cal2.cbs)
#contour plots
dc.PlotLogLikelihoodContours(pnbd.cbs.LL, params2, cal.cbs = cal2.cbs , n.divs = 5,
                             num.contour.lines = 7, zoom.percent = 0.3,
                             allow.neg.params = FALSE, param.names = param.names)
#heterogeneity of open
pnbd.PlotTransactionRateHeterogeneity(params2, lim = NULL)
#heterogeneity of drop out
pnbd.PlotDropoutRateHeterogeneity(params2)

#individual predicitions - 2 month (60 day) period - new customer
pnbd.Expectation(params2, t = 60) # 5.961658 days with AppOpen events

#individual predictions - 2 month period - existing customer
cal2.cbs["1420",]
x <- cal2.cbs["1420", "x"]         
t.x <- cal2.cbs["1420", "t.x"]     
T.cal <- cal2.cbs["1420", "T.cal"]
pnbd.ConditionalExpectedTransactions(params2, T.star = 60, 
                                     x, t.x, T.cal)

#probabilities of customers being alive

x          
t.x        
#end of calibration
T.cal <- 60
pnbd.PAlive(params2, x, t.x, T.cal)
p.alives <- pnbd.PAlive(params2, cal2.cbs[,"x"], cal2.cbs[,"t.x"], cal2.cbs[,"T.cal"])
ggplot(as.data.frame(p.alives),aes(x=p.alives))+
  geom_histogram(colour="grey",fill="orange")+
  ylab("Number of Customers")+
  xlab("Probability Customer is 'Live'")+
  theme_minimal()
pnbd.PlotFrequencyInCalibration(params2, cal2.cbs, 
                                censor=8, title="Model vs. Reality during Calibration")
#assess model in holdout period
x.star   <- dataSpringM[[2]][[2]][,1]
cal2.cbs <- cbind(cal2.cbs, x.star)
str(cal2.cbs)

holdoutdates <- attributes(dataSpringM[[2]][[1]])[[2]][[2]]
holdoutlength <- round(as.numeric(max(as.Date(holdoutdates))-
                                    min(as.Date(holdoutdates)))/7)

T.star <- holdoutlength
censor <- 7 
comp <- pnbd.PlotFreqVsConditionalExpectedFrequency(params2, T.star,
                                                    cal2.cbs, x.star, censor)
```

### Attribution
This code is revised from emarkou on github https://github.com/blendo-app/NBD-Pareto-churn-model using the BTYD package. This flow is emulating @drob's Tidy Tuesday style
