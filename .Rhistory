data <- read.csv('./Data/NBDparetoFebMay.csv')
# Convert time in ms to dates by day
data$open_time <- format(as.POSIXct(data$open_time / 1000, origin = "1970-01-01", tz = "America/Los_Angeles"), "%Y%m%d")
data$open_time <- as.Date(data$open_time, "%Y%m%d")
head(data)
min(data$open_time)
max(data$open_time)
data %>%
filter(open_time < "2018-06-01" & open_time > "2018-01-31")
dim(data)
data <- read.csv('./Data/NBDparetoFebMay.csv')
# Convert time in ms to dates by day
data$open_time <- format(as.POSIXct(data$open_time / 1000, origin = "1970-01-01", tz = "America/Los_Angeles"), "%Y%m%d")
data$open_time <- as.Date(data$open_time, "%Y%m%d")
length(unique(data$uid))
data2 <- tibble::rowid_to_column(data, "ID")
rm(data2)
users <- unique(data$uid)
users <- unique(data$uid) %>%
tibble::rowid_to_column(users, "ID")
users <- as.data_frame(unique(data$uid)) %>%
tibble::rowid_to_column(users, "ID")
users <- as.data.frame(unique(data$uid)) %>%
tibble::rowid_to_column(users, "ID")
users <- as.data.frame(unique(data$uid)) %>%
mutate(ID = seq.int(nrow(users)))
nrow(users)
is.data.frame(users)
users <- as.data.frame(unique(data$uid))
nrow(users)
rm(users)
users <- as.data.frame(unique(data$uid)) %>%
mutate(ID = seq.int(nrow(users)))
users <- unique(data$uid) %>%
as.data.frame() %>%
mutate(ID = seq.int(nrow(users)))
users <- unique(data$uid)
rm(users)
users <- unique(data$uid) %>%
as.data.frame()
rum(users)
rm(users)
users <- unique(data$uid) %>%
as.data.frame() %>%
mutate(ID = seq.int(nrow(users)))
users <- unique(data$uid) %>%
as.data.frame() %>%
mutate(ID = seq.int(nrow()))
users <- unique(data$uid) %>%
as.data.frame() %>%
mutate(ID = seq.int(length(unique(data$uid))))
users <- unique(data$uid) %>%
as.data.frame() %>%
mutate(uid = seq.int(length(unique(data$uid))))
users <- unique(data$uid) %>%
as.data.frame(col.names = c("uid")) %>%
mutate(uid = seq.int(length(unique(data$uid))))
users <- unique(data$uid) %>%
as.data.frame(col.names = c("uid")) %>%
mutate(ID = seq.int(length(unique(data$uid))))
class(unique(data$uid))
is.vector(unique(data$uid))
is.list(unique(data$uid))
users <- unique(as.character(data$uid)) %>%
as.data.frame(col.names = c("uid")) %>%
mutate(ID = seq.int(length(unique(data$uid))))
ckass(unique(as.character(data$uid)))
class(unique(as.character(data$uid)))
type(unique(as.character(data$uid)))
is.list(unique(as.character(data$uid)))
users <- unique(data$uid) %>%
as.data.frame() %>%
names() = c("uid") %>%
mutate(ID = seq.int(length(unique(data$uid))))
users <- unique(data$uid) %>%
as.data.frame() %>%
names() = c("uid")
users <- unique(data$uid) %>%
as.data.frame() %>%
mutate(
uid = .,
ID = seq.int(length(unique(data$uid)))
)
users <- unique(data$uid) %>%
as.data.frame() %>%
mutate(uid = ., ID = seq.int(length(unique(data$uid))))
?rm(users)
rm(users)
users <- unique(data$uid) %>%
as.data.frame() %>%
mutate(uid = ., ID = seq.int(length(unique(data$uid))))
users <- unique(data$uid) %>%
as.data.frame() %>%
mutate(ID = seq.int(length(unique(data$uid))))
users <- unique(data$uid) %>%
as.data.frame() %>%
mutate(ID = seq.int(length(unique(data$uid))), uid = .)
users <- unique(data$uid) %>%
as.data.frame() %>%
mutate(ID = seq.int(length(unique(data$uid)))) %>%
rename(. = uid)
users <- unique(data$uid) %>%
as.data.frame() %>%
mutate(ID = seq.int(length(unique(data$uid)))) %>%
rename(uid = .)
is.data.frame(data$uid)
is.list(data$uid)
is.vector(data$uid)
is.array(data$uid)
is.matrix(data$uid)
View(data$uid)
users$.
names(users)
names(users)[1] <- "uid"
data <- merge(datas, users, by = "uid")
data <- merge(data, users, by = "uid")
length(unique(data$ID))
data <- data[order(data$uid, data$open_time),]
head(data)
sapply(data, function(y) sum(length(which(is.na(y)))))
summary(count(data$id))
summary(count(data$uid))
summary(count(data$ID))
summary(count(data$open_time))
count(data$open_time
)
library(reshape)
library(BTYD)
library(ggplot2)
library(plyr)
library(lubridate)
# Import the csv file
data <- read.table("./Data/NBDpareto.csv", fill = TRUE, header = TRUE, sep = ",", na.strings = c("NA","")) # 605298 2
# Order by user and time
data <- data[order(data$uid, data$open_time),]
# Convert time in ms to dates by day
data$open_time <- format(as.POSIXct(data$open_time / 1000, origin = "1970-01-01", tz = "America/Los_Angeles"), "%Y%m%d")
data$open_time <- as.Date(data$open_time, "%Y%m%d")
#assign ids to users (if you must)
users <- unique(data$uid) %>%
as.data.frame() %>%
mutate(ID = seq.int(length(unique(data$uid))))
library(magrittr)
#assign ids to users (if you must)
users <- unique(data$uid) %>%
as.data.frame() %>%
mutate(ID = seq.int(length(unique(data$uid))))
names(users)[1] <- "uid"
data <- merge(data, users, by = "uid")
# NA Values
sapply(data, function(y) sum(length(which(is.na(y))))) # 1752 values all between 8-23 and 8-28 (10% of that week. Perhaps an error that week?)
# I will remove, but probably should impute with some sort RF
data <- data[-which(is.na(data$uid)),] # 603546      3
summary(count(data$id))
summary(count(data$date))
count(data$ID)
summary(count(data$id["freq"]))
knitr::opts_chunk$set(echo = TRUE)
app_opens <- read.csv('./Data/NBDparetoFebMay.csv')
# Convert time in ms to dates by day
names(app_opens)[2] <- "date"
app_opens$open_time <- format(as.POSIXct(app_opens$open_time / 1000, origin = "1970-01-01", tz = "America/Los_Angeles"), "%Y%m%d")
app_opens$open_time <- format(as.POSIXct(app_opens$date / 1000, origin = "1970-01-01", tz = "America/Los_Angeles"), "%Y%m%d")
app_opens$open_time <- as.Date(app_opens$date, "%Y%m%d")
app_opens <- read.csv('./Data/NBDparetoFebMay.csv')
# Convert time in ms to dates by day
names(app_opens)[2] <- "date"
app_opens$open_time <- format(as.POSIXct(app_opens$date / 1000, origin = "1970-01-01", tz = "America/Los_Angeles"), "%Y%m%d")
app_opens <- read.csv('./Data/NBDparetoFebMay.csv')
app_opens <- read.csv('./Data/NBDparetoFebMay.csv')
# Convert time in ms to dates by day
names(app_opens)[2] <- "date"
# Convert time in ms to dates by day
names(app_opens)[2] <- "date"
app_opens$date <- format(as.POSIXct(app_opens$date / 1000, origin = "1970-01-01", tz = "America/Los_Angeles"), "%Y%m%d")
app_opens$date <- as.Date(app_opens$date, "%Y%m%d")
length(unique(app_opens$uid))
users <- unique(app_opens$uid) %>%
as.data.frame() %>%
mutate(id = seq.int(length(unique(app_opens$uid))))
names(users)[1] <- "uid"
app_opens <- merge(app_opens, users, by = "uid")
app_opens <- merge(app_opens, users, by = "uid")
app_opens <- app_opens[order(app_opens$uid, app_opens$open_time),]
app_opens <- app_opens[order(app_opens$uid, app_opens$date),]
sapply(app_opens, function(y) sum(length(which(is.na(y))))) # None - nice
summary(count(app_opens$open_time))
summary(count(app_opens$open_time)['freq'])
count(app_opens$open_time)
count(app_opens$date)
count_datte <- count(app_opens$date)
rm(count_datte)
count_date <- count(app_opens$date)
library(tidyverse)
library(BTYD)
library(lubridate)
app_opens <- read.csv('./Data/NBDparetoFebMay.csv')
app_opens <- read.csv('./Data/NBDparetoFebMay.csv')
# Convert time in ms to dates by day
names(app_opens)[2] <- "date"
app_opens$date <- format(as.POSIXct(app_opens$date / 1000, origin = "1970-01-01", tz = "America/Los_Angeles"), "%Y%m%d")
app_opens$date <- as.Date(app_opens$date, "%Y%m%d")
length(unique(app_opens$uid))
users <- unique(app_opens$uid) %>%
as.data.frame() %>%
mutate(id = seq.int(length(unique(app_opens$uid))))
names(users)[1] <- "uid"
app_opens <- merge(app_opens, users, by = "uid")
app_opens <- merge(app_opens, users, by = "uid")
app_opens <- app_opens[order(app_opens$uid, app_opens$date),]
app_opens <- read.csv('./Data/NBDparetoFebMay.csv')
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(BTYD)
library(lubridate)
app_opens <- read.csv('./Data/NBDparetoFebMay.csv')
app_opens <- read.csv('./Data/NBDparetoFebMay.csv')
# Convert time in ms to dates by day
names(app_opens)[2] <- "date"
app_opens$date <- format(as.POSIXct(app_opens$date / 1000, origin = "1970-01-01", tz = "America/Los_Angeles"), "%Y%m%d")
app_opens$date <- as.Date(app_opens$date, "%Y%m%d")
length(unique(app_opens$uid))
users <- unique(app_opens$uid) %>%
as.data.frame() %>%
mutate(id = seq.int(length(unique(app_opens$uid))))
app_opens <- merge(app_opens, users, by = "uid") %>%
arrange(asc(uid), asc(date))
names(users)[1] <- "uid"
app_opens <- merge(app_opens, users, by = "uid") %>%
arrange(asc(uid), asc(date))
app_opens <- merge(app_opens, users, by = "uid") %>%
arrange(uid, date)
head(app_opens)
sapply(app_opens, function(y) sum(length(which(is.na(y))))) # None - nice
(count_date <- count(app_opens$date))
count_date <- count(app_opens$date)
count(app_opens$date)
library(reshape)
library(BTYD)
library(ggplot2)
library(plyr)
library(lubridate)
library(magrittr)
# Import the csv file
data <- read.table("./Data/NBDpareto.csv", fill = TRUE, header = TRUE, sep = ",", na.strings = c("NA","")) # 605298 2
# Order by user and time
data <- data[order(data$uid, data$open_time),]
# Convert time in ms to dates by day
data$open_time <- format(as.POSIXct(data$open_time / 1000, origin = "1970-01-01", tz = "America/Los_Angeles"), "%Y%m%d")
# Import the csv file
data <- read.table("./Data/NBDpareto.csv", fill = TRUE, header = TRUE, sep = ",", na.strings = c("NA","")) # 605298 2
# Order by user and time
data <- data[order(data$uid, data$open_time),]
# Import the csv file
data <- read.table("./Data/NBDpareto.csv", fill = TRUE, header = TRUE, sep = ",", na.strings = c("NA","")) %>% # 605298 2
arrange(uid, open_time)
head(data)
# Convert time in ms to dates by day
data$open_time <- format(as.POSIXct(data$open_time / 1000, origin = "1970-01-01", tz = "America/Los_Angeles"), "%Y%m%d")
data$open_time <- as.Date(data$open_time, "%Y%m%d")
#assign ids to users
users <- unique(data$uid) %>%
as.data.frame() %>%
mutate(id = seq.int(length(unique(data$uid))))
names(users)[1] <- "uid"
data <- merge(data, users, by = "uid")
# NA Values
sapply(data, function(y) sum(length(which(is.na(y))))) # 1752 values all between 8-23 and 8-28 (10% of that week. Perhaps an error that week?)
names(data)[2] <- "date"
# I will remove, but probably should impute with some sort RF
data <- data[-which(is.na(data$uid)),] # 603546      3
summary(count(data$id["freq"]))
count(data$id)
summarize(count(data$id))
summarize(count(data$id)['freq'])
count(app_opens$date)
count(app_opens$id)
count_date <- count(app_opens$date)
count_id <- count(app_opens$id)
ggplot(count_date, aes(x=freq)) +
geom_hist()
ggplot(count_date, aes(x=freq)) +
geom_histogram()
ggplot(app_opens$date) +
geom_histogram()
ggplot(app_opens, aes(x=date)) +
geom_histogram()
app_opens %>%
ggplot(aes(date)) +
geom_histogram()
app_opens %>%
ggplot(aes(date)) +
geom_histogram()
app_opens %>%
ggplot(aes(date)) +
geom_histogram()
app_opens %>%
ggplot(aes(date)) +
geom_histogram()
theme_set(theme_light())
app_opens %>%
ggplot(aes(date)) +
geom_histogram()
app_opens %>%
ggplot(aes(date)) +
geom_histogram()
app_opens %>%
ggplot(aes(id)) +
geom_histogram()
count_date %>%
ggplot(aes(freq)) +
geom_point()
count_date %>%
ggplot(aes(y=freq)) +
geom_point()
count_date %>%
ggplot(aes(x=x, y=freq)) +
geom_point()
count_date %>%
ggplot(aes(x=x, y=freq)) +
geom_point() +
geom_smooth(method = 'lm' )
count1 <- count(data$id)[-34060,]
data1 <- data[which(data$id != 34060),] # remove this power user
summary(count1)
summary(count(data1$date))
ggplot(count1,aes(x=x, y=freq))+
geom_point(colour="black")+
ylab("AppOpen Frequency")+
xlab("User id")+
theme_minimal()
ggplot(count(data1$date),aes(x=x, y=freq))+
geom_point(colour="black")+
ylab("AppOpen Frequency")+
xlab("Date")+
theme_minimal()
ggplot(count(data1$date),aes(x=x, y=freq))+
geom_point(colour="black")+
ylab("AppOpen Frequency")+
xlab("Date")+
geom_smooth(method = 'lm' )+
theme_minimal()
count_date %>%
ggplot(aes(x=x, y=freq)) +
geom_point() +
geom_smooth(method = 'lm' )
elogSpring <- app_opens[, c("id", "date")]
names(elogSpring)[1] <- "cust"
elogSpringM <- dc.MergeTransactionsOnSameDate(elogSpring)
theme_set(theme_light())
openFreq <- ddply(elogSpring, .(cust), summarize,
daysBetween = as.numeric(diff(date)))
ggplot(openFreq,aes(x=daysBetween))+
geom_histogram(fill="orange")+
xlab("Time between opens (days)")
library(scales)
options(scipen = 5)
ggplot(openFreq,aes(x=daysBetween))+
geom_histogram(fill="orange")+
xlab("Time between opens (days)")
elogSpring$date<-as.POSIXct(elogSpring$date)
# splitting data
(end.of.cal.period <- min(elogSpring$date)+(max(elogSpring$date)-min(elogSpring$date))/2)
dataSpring <- dc.elogToCbsCbt(elogSpring, per="day",
T.cal=end.of.cal.period,
statistic = "freq")
library(BTYD)
dataSpring <- dc.ElogToCbsCbt(elogSpring, per="day",
T.cal=end.of.cal.period,
statistic = "freq")
openFreq <- ddply(elogSpringM, .(cust), summarize,
daysBetween = as.numeric(diff(date)))
ggplot(openFreq,aes(x=daysBetween))+
geom_histogram(fill="orange")+
xlab("Time between opens (days)")
cal2.cbs <- as.matrix(dataSpring[[1]][[1]])
(params2 <- pnbd.EstimateParameters(cal2.cbs))
dataSpringM <- dc.ElogToCbsCbt(elogSpringM, per="day",
T.cal=end.of.cal.period,
statistic = "freq")
cal2.cbs <- as.matrix(dataSpringM[[1]][[1]])
(params2 <- pnbd.EstimateParameters(cal2.cbs))
str(cal2.cbs)
View(cal2.cbs)
(LL <- pnbd.cbs.LL(params2, cal2.cbs))
# it is a good idea to make a few more estimates to see if they converge
p.matrix <- c(params2, LL)
for (i in 1:5) {
params2 <- pnbd.EstimateParameters(cal2.cbs, params2)
LL <- pnbd.cbs.LL(params2, cal2.cbs)
p.matrix.row <- c(params2, LL)
p.matrix <- rbind(p.matrix, p.matrix.row)
}
p.matrix
(params2 <- p.matrix[dim(p.matrix)[1],1:4])
param.names <- c("r", "alpha", "s", "beta")
LL <- pnbd.cbs.LL(params2, cal2.cbs)
#contour plots
dc.PlotLogLikelihoodContours(pnbd.cbs.LL, params2, cal.cbs = cal2.cbs , n.divs = 5,
num.contour.lines = 7, zoom.percent = 0.3,
allow.neg.params = FALSE, param.names = param.names)
data1718 <- read_csv("./Data/ChurnSeg_Past_6_months_.csv")
#heterogeneity of open
pnbd.PlotTransactionRateHeterogeneity(params2, lim = NULL)
#heterogeneity of drop out
pnbd.PlotDropoutRateHeterogeneity(params2)
#individual predicitions - 2 (or 60 days) month period - new customer
pnbd.Expectation(params2, t = 60) # 5.462855 days with AppOpen events
#individual predictions - 2 month period - existing customer
cal2.cbs["42",]
#individual predictions - 2 month period - existing customer
cal2.cbs["420",]
#individual predictions - 2 month period - existing customer
cal2.cbs["1420",]
x <- cal2.cbs["1420", "x"]
t.x <- cal2.cbs["1420", "t.x"]
T.cal <- cal2.cbs["1420", "T.cal"]
pnbd.ConditionalExpectedTransactions(params2, T.star = 60,
x, t.x, T.cal)
#end of calibration
T.cal <- 61.5
pnbd.PAlive(params2, x, t.x, T.cal)
#end of calibration
T.cal <- 60
pnbd.PAlive(params2, x, t.x, T.cal)
p.alives <- pnbd.PAlive(params2, cal2.cbs[,"x"], cal2.cbs[,"t.x"], cal2.cbs[,"T.cal"])
ggplot(as.data.frame(p.alives),aes(x=p.alives))+
geom_histogram(colour="grey",fill="orange")+
ylab("Number of Customers")+
xlab("Probability Customer is 'Live'")+
theme_minimal()
pnbd.PlotFrequencyInCalibration(params2, cal2.cbs,
censor=10, title="Model vs. Reality during Calibration")
pnbd.PlotFrequencyInCalibration(params2, cal2.cbs,
censor=8, title="Model vs. Reality during Calibration")
#assess model in holdout period
x.star   <- dataSpringM[[2]][[2]][,1]
cal2.cbs <- cbind(cal2.cbs, x.star)
str(cal2.cbs)
holdoutdates <- attributes(dataSpringM[[2]][[1]])[[2]][[2]]
holdoutlength <- round(as.numeric(max(as.Date(holdoutdates))-
min(as.Date(holdoutdates)))/7)
T.star <- holdoutlength
censor <- 7
comp <- pnbd.PlotFreqVsConditionalExpectedFrequency(params2, T.star,
cal2.cbs, x.star, censor)
View(cal2.cbs)
View(cal2.cbs)
str(data1718)
data1718 %>%
ggplot(aes(value.minutes_played))+
geom_histogram()
data1718 %>%
ggplot(aes(value.minutes_played))+
geom_histogram()+
scale_x_log10()
# ===== Data Cleaning =====
# Check NAs
sapply(df, function(y) sum(length(which(is.na(y))))) # these are for blanks that have been converted to NA
# Correlation plot for numeric variables
data1718.n<-sapply(data1718,class)=='numeric' | sapply(data1718,class)=='integer' # numeric columns
data1718Num<-data1718[,data1718.n] # subset numeric
corr<-cor(data1718Num)
corrplot(corr, type = "lower", tl.pos = "ld")
library(corrplot)
corrplot(corr, type = "lower", tl.pos = "ld")
corr[ 1:16, 2]
library(tidyverse)
library(corrplot)
library(randomForest)
library(sqldf)
library(glmnet)
library(caret)
data1718 <- read_csv("./Data/ChurnSeg_Past_6_months_.csv")
# ===== Data Cleaning =====
# Check NAs
sapply(data1718, function(y) sum(length(which(is.na(y))))) # these are for blanks that have been converted to NA
dim(data1718)
min(data1718$value.sub_Start)
# Converting value.sub_Start and value.sub_End to Date (exclude time)
data1718$value.sub_End <- as.Date(data1718$value.sub_End, "%Y-%m-%d");
data1718$value.sub_Start <- as.Date(data1718$value.sub_Start, format="%Y-%m-%d")
min(data1718$value.sub_Start)
summary(data1718$value.sub_Start)
data1718 %>%
arrange(value.sub_Start)
dim(data1718)
data1718 %>%
filter(value.sub_Start >= "2017-08-15")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(corrplot)
library(randomForest)
library(sqldf)
"2017-08-15" + 35
as.Date("2017-08-15") + 35
cohorts <-
date1 <- as.Date("2017-08-15") + TIME_WINDOW
TIME_WINDOW <- 35 # 5 weeks
cohorts <-
date1 <- as.Date("2017-08-15") + TIME_WINDOW
cohort1 <- data1718 %>%
filter(value.sub_Start < date1)
dim(cohort1)
sapply(cohort1, function(y) sum(length(which(is.na(y)))))
cohorts <- (as.Date("2017-08-15") - as.Date("2018-08-15")) / TIME_WINDOW
cohort
cohorts
cohorts <- as.integer(as.Date("2017-08-15") - as.Date("2018-08-15")) / TIME_WINDOW
cohorts
cohorts <- floor(as.integer(as.Date("2018-08-15") - as.Date("2017-08-15")) / TIME_WINDOW)
cohorts
